setwd("Dropbox/College/4_Fourth_Year/Capstone/chetan_mishra_sandbox/semantic range modeling/")
source("methods.R")
sourceCpp("cooccurences.cpp")
library(microbenchmark)
test_dir("tests")
data_wbc <- VCorpus(DirSource("../webscraping westboro/sermons/"))
processed_wbc <- preprocessing(data_wbc, stpwds, wordStem)
set_sepstring(occurrences_sepstr)
load_string_k(processed_wbc[[1]]$content, 2)
load_string_sentence(processed_wbc[[1]]$content)
str(map_info())
samp <- sample.int(12584, 20)
retest <- function() {
reload_cooccurrences()
print(paste0("subject: ", str_extract_cpp("subject", "subject")))
print(paste0("sub: ", str_extract_cpp("subject", "sub")))
print(paste0("subje: ", str_extract_cpp("subject", "sub.{2}")))
}
# why did i spend so long on this?
load(file="timing_results.RData")
setwd("Dropbox/College/4_Fourth_Year/Capstone/chetan_mishra_sandbox/semantic range modeling/")
getwd()
load(file="timing_results.RData")
list.files()
library(tm)
source("methods.R")
data_wbc <- VCorpus(DirSource("../webscraping westboro/sermons/"))
processed_wbc <- preprocessing(data_wbc, stpwds, wordStem)
?TermDocumentMatrix
?removePunctuation
bySentence
?words
class(tokenize=T)
test <- True
test <- T
class(test)
"logical" %in% class(test)
!("logical" %in% class(test))
!("logical" %in% class(test))
test <- F
!("logical" %in% class(test))
!("logical" %in% class(test))
!("logical" %in% class(test))
smaller_graph
setwd("Dropbox/College/4_Fourth_Year/Capstone/chetan_mishra_sandbox/semantic range modeling/")
source("methods.R")
library(microbenchmark)
headwords <- c("god", "love", "infidel", "jewish", "jew", "hate", "evil", "ignore", "justice",
"islamic", "muslim") # how to stem
cooccurrences <- dir_to_cooccurrences("../webscraping sermoncentral/john_piper/")
# param1 is alpha and param2 is beta. look at ACOM by ji and ploux for an explanation of these
# parameters
wordtable <- cooccurrences[,.(target, context, weight=as.integer(as.character(freq)))]
param1 <- 0.05
param2 <- 0.10
param3 <- 0.05
word <- "love"
complete_net <- graph.data.frame(wordtable[sample.int(nrow(wordtable), 1000000)], directed=T)
subset_net <- delete_weak_links(complete_net, word, param1)
# E(subset_net)[from(word)]
candidate_contextonyms <- neighborhood(subset_net, 1, V(subset_net)[name == word], mode="out")
larger_graph <- subset_net
for(vertex_index in candidate_contextonyms) {
larger_graph <- delete_weak_links(larger_graph,
V(larger_graph)[vertex_index]$name, param2)
}
nodes_to_keep <- neighborhood(larger_graph, 2, word, "out")[[1]]
larger_graph <- induced.subgraph(larger_graph, nodes_to_keep, "auto")
larger_graph <- delete.edges(larger_graph, E(larger_graph)[from(nodes_to_keep)])
smaller_graph <- subset_net
vertexes_to_delete <- c()
for(vertex_index in candidate_contextonyms) {
smaller_graph <- delete_weak_links(smaller_graph, V(smaller_graph)[vertex_index]$name, param3)
if (get.edge.ids(smaller_graph, c(word, V(smaller_graph)[vertex_index]$name)) == 0) {
vertexes_to_delete <- c(vertexes_to_delete, vertex_index)
}
}
cooccurrences <- dir_to_cooccurrences("../webscraping sermoncentral/john_piper/")
